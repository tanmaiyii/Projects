{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                      NLP Assignment - 1\n",
    "\n",
    "\n",
    "## Part A - Deception Detection\n",
    "\n",
    "\n",
    "### 4. Improving Preprocessing for better accuracy\n",
    "After training the classifer, different ways have been implemented to improve the preprocessing and see the effect on the classifer performance. I have implemented the TfIdf Vectorizer , CountVectorizer with Stop words and the regex tokenizer to tokenise punctuations. Also implemented the Snowball Stemmer and Lemmatizer. Finally, also implemented the effect of using bigrams instead of unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  \n",
    "import unicodecsv   # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "# libraries for: regular expressions, file I/O\n",
    "import sys\n",
    "from sklearn.metrics import * # to report on precision and recall\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from textstat.textstat import textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path, 'rb') as f:\n",
    "        reader = unicodecsv.reader(f, encoding='utf-8', delimiter='\\t')\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            (Id, Text, Label) = parseReview(line)\n",
    "            rawData.append((Id, Text, Label))\n",
    "            preprocessedData.append((Id, preProcess(Text), Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    for (_, Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## QUESTION 4##\n",
    "################\n",
    "\n",
    "# # Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    Id = reviewLine[0]\n",
    "    Text = reviewLine[8]\n",
    "    Label = reviewLine[1]\n",
    "    return (Id, Text, Label)\n",
    "\n",
    "# TEXT PREPROCESSING\n",
    "\n",
    "# def preProcess(text):\n",
    "#     #normalisation and tokenising \n",
    "#     no_symbols = re.sub(r'[^\\w]', ' ', text.lower())\n",
    "#     tokens = no_symbols.split()\n",
    "#     return tokens\n",
    "#\n",
    "# To remove stopwords\n",
    "# def preProcess(text):\n",
    "#     vectorizer = CountVectorizer(stop_words = 'english')\n",
    "#     analyze = vectorizer.build_analyzer()\n",
    "#     tokens=analyze(text)\n",
    "#     return tokens\n",
    "\n",
    "# def preProcess(text):\n",
    "#     tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     return tokens\n",
    "\n",
    "# To stem the data\n",
    "# import nltk.stem\n",
    "# sbs = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# def preProcess(text):\n",
    "#     vectorizer = CountVectorizer(stop_words = 'english')\n",
    "#     analyze = vectorizer.build_analyzer()\n",
    "#     tokens=analyze(text)\n",
    "#     tokens = [sbs.stem(t) for t in tokens]\n",
    "# # Should return a list of tokens\n",
    "#     return tokens\n",
    "\n",
    "# # # To lemmatize the data\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# wnl = WordNetLemmatizer()\n",
    "\n",
    "# def preProcess(text):\n",
    "#     vectorizer = CountVectorizer(stop_words = 'english')\n",
    "#     analyze = vectorizer.build_analyzer()\n",
    "#     tokens=analyze(text)\n",
    "#     tokens = [wnl.lemmatize(t) for t in tokens]\n",
    "# # #Should return a list of tokens\n",
    "#     return tokens\n",
    "\n",
    "# import nltk\n",
    "# from nltk import bigrams\n",
    "# def preProcess(text):\n",
    "#     #normalisation and tokenising \n",
    "#     no_symbols = re.sub(r'[^\\w]', ' ', text.lower())\n",
    "#     tokens = no_symbols.split()\n",
    "#     bitokens = list(bigrams(tokens))\n",
    "#     return bitokens\n",
    "\n",
    "import nltk\n",
    "from nltk import trigrams\n",
    "def preProcess(text):\n",
    "    #normalisation and tokenising \n",
    "    no_symbols = re.sub(r'[^\\w]', ' ', text.lower())\n",
    "    tokens = no_symbols.split()\n",
    "    tritokens = list(trigrams(tokens))\n",
    "    return tritokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## QUESTION 2 ##\n",
    "################\n",
    "# featureDict = {} # A global dictionary of features\n",
    "# def toFeatureVector(words):\n",
    "#     v = {}\n",
    "#     for w in words:\n",
    "#         try:\n",
    "#             i = featureDict[w]\n",
    "#         except KeyError:\n",
    "#             i = len(featureDict) + 1\n",
    "#             featureDict[w] = i\n",
    "#         try:\n",
    "#             v[w] += (1.0/len(words))\n",
    "#         except KeyError:\n",
    "#             v[w] = (1.0/len(words))\n",
    "#     return v\n",
    "\n",
    "# def toFeatureVector(words):\n",
    "#     featureVector = {}\n",
    "#     # return a dictionary 'featureVect' where the keys are the tokens in 'words' and the values are the number of occurrences of the tokens\n",
    "#     for w in words:\n",
    "#         newW = '{}-{}'.format(w[0],w[1])\n",
    "#         #print(newW)\n",
    "#         #print(w)\n",
    "#         #try:\n",
    "#         featureVector[newW] = featureVector.get(newW,0) + 1\n",
    "#         #except KeyError:\n",
    "#         #    featureVector[w] = 1.0\n",
    "#         #try:\n",
    "#         featureDict[newW] = featureDict.get(newW,0) + 1\n",
    "#         #except KeyError:\n",
    "#         #    featureDict[w] = 1.0\n",
    "#     return featureVector\n",
    "\n",
    "def toFeatureVector(words):\n",
    "    featureVector = {}\n",
    "    # return a dictionary 'featureVect' where the keys are the tokens in 'words' and the values are the number of occurrences of the tokens\n",
    "    for w in words:\n",
    "        newW = '{}-{}-{}'.format(w[0],w[1],w[2])\n",
    "        #print(newW)\n",
    "        #print(w)\n",
    "        #try:\n",
    "        featureVector[newW] = featureVector.get(newW,0) + 1\n",
    "        #except KeyError:\n",
    "        #    featureVector[w] = 1.0\n",
    "        #try:\n",
    "        featureDict[newW] = featureDict.get(newW,0) + 1\n",
    "        #except KeyError:\n",
    "        #    featureDict[w] = 1.0\n",
    "    return featureVector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## QUESTION 3 ##\n",
    "################\n",
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)\n",
    "\n",
    "\n",
    "myTestData = []\n",
    "myTrainData = []\n",
    "\n",
    "    \n",
    "def crossValidate(dataset, folds):\n",
    "    cv_results = []\n",
    "    accuracy = []\n",
    "    shuffle(dataset)\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
    "        print (\"fold start %d foldSize %d\" % (i, foldSize))\n",
    "        myTestData = dataset[i:i+foldSize]\n",
    "        myTrainData = dataset[0:i] + dataset[i+foldSize:]\n",
    "        classifier = trainClassifier(myTrainData)\n",
    "        y_pred = predictLabels(myTestData, classifier)\n",
    "#         review,label = zip(*myTestData)\n",
    "#         y_true = label\n",
    "        y_true = [x[1] for x in myTestData]\n",
    "#         y_true = classifier.classify(map(lambda x: x[1], myTestData))\n",
    "        cv_results.append(precision_recall_fscore_support(y_true, y_pred, average='weighted'))\n",
    "        accuracy.append(accuracy_score(y_true, y_pred))\n",
    "        \n",
    "#Calclualte avergae of values over the 10-fold runs\n",
    "    cv_results = np.asarray(cv_results)\n",
    "    cv_results = [np.mean(cv_results[:,0]), np.mean(cv_results[:,1]), np.mean(cv_results[:,2])]\n",
    "    \n",
    "    accuracy = np.asarray(accuracy)\n",
    "    accuracy = np.mean(accuracy)\n",
    "    cv_results.append(accuracy)\n",
    "    \n",
    "    return cv_results\n",
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the above functions, the loadData function is called to load the data. Consequently the functions defined above have been called. The corresponding accuracy scores have been calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "1410287\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'amazon_reviews.txt'\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath)\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold start 0 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 1680 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 3360 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 5040 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 6720 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 8400 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 10080 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 11760 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 13440 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 15120 foldSize 1680\n",
      "Training Classifier...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6019586340824624,\n",
       " 0.6014880952380952,\n",
       " 0.6014834355322087,\n",
       " 0.6014880952380952]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossValidate(trainData, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
